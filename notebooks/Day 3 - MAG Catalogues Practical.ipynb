{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3be1d41e-2365-4c09-84c2-84e177cf254c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SymbNet Course - EBI MGnify 2022\n",
    "## MGnify Genomes resource - Metagenomic Assembled Genomes Catalogues - Practical exercise\n",
    "\n",
    "### Aims\n",
    "In this exercise, we will learn how to use the [Genomes resource within MGnify](https://www.ebi.ac.uk/metagenomics/browse#genomes).\n",
    "\n",
    "- Discover the available data on the MGnify website\n",
    "- Use two search mechanisms (search by _gene_ and search by _genome_)\n",
    "- Learn how to use the MGnify API to fetch data using scripts or analysis notebooks\n",
    "- Use the _genome_ search mechanism via the API, to compare your own MAGs against a MGnify catalogue and see whether they are novel\n",
    "\n",
    "### How this works\n",
    "This file is a [Jupyter Notebook](https://jupyter.org). \n",
    "It has instructions, and also code cells. The code cells are connected to Python, and you can run all of the code in a cell by pressing Play (▶) icon in the top bar, or pressing `shift + return`.\n",
    "The code libraries you should need are already installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704e24e-b25c-4c5e-b02d-ecd4f23d085c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a639807-3595-43a8-8d36-eaddd42adb23",
   "metadata": {
    "tags": []
   },
   "source": [
    "[pandas](https://pandas.pydata.org/docs/reference/index.html#api) is a data analysis library with a huge list of features. It is very good at holding and manipulating table data. It is almost always short-handed to `pd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb916f-02e9-46db-9013-01d63516ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e897ec25-52af-4e2b-9d90-e5c2e03da580",
   "metadata": {},
   "source": [
    "[jsonapi-client](https://pypi.org/project/jsonapi-client/) is a library to get formatted data from web services into python code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3effca5-84e9-4890-baad-20baf8073f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from jsonapi_client import Session as APISession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8408a1d1-6417-4821-8d63-20727b7e0d01",
   "metadata": {},
   "source": [
    "[matplotlib](https://matplotlib.org) is the go-to package for making plots and charts. It is almost always short-handed to `plt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffe3d7a-7f0e-4900-98fa-d6580051d66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37498fe-595c-475f-8989-62af8f1fa53f",
   "metadata": {},
   "source": [
    "`pathlib` is part of the Python standard library. We use it to find files and directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97131f2b-8804-4fdc-9ae6-3c5300cb7478",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a553b808-c094-4dfb-b558-9efbd0bb0c93",
   "metadata": {},
   "source": [
    "`time` is part of the Python standard library. We will use it to wait for results from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e18e4a-81f4-46fe-8393-7df2f0dec8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c875216-628c-4256-89de-e4341a9c0804",
   "metadata": {},
   "source": [
    "`tarfile` is part of the Python standard library. We will use it to extract compressed files from a `.tar.gz` file that the API gives us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044d638-d715-4597-b809-2d53d4fab74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94dd903-6309-40f2-b11c-6c54edb6cf4a",
   "metadata": {},
   "source": [
    "**We will also import some extra package later. `sourmash` and `requests` will be used for specialised tasks and explained at the time.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa11014-51f3-4b5c-8e62-1256e70a2423",
   "metadata": {
    "tags": []
   },
   "source": [
    "# The MGnify API (summary of what is covered in [the appendix](<./Appendix - More Exercises Using the MGnify API.ipynb>))\n",
    "<span style=\"background-color:#ffaaaa; padding: 8px\">In a hurry? [⇓ Skip to the tasks](#Task-1---list-Genome-Catalogues).</span>\n",
    "\n",
    "## Core concepts\n",
    "An [API](https://en.wikipedia.org/wiki/API \"Application programming interface\") is how your scripts (e.g. Python or R) can talk to the MGnify database.\n",
    "\n",
    "The MGnify API uses [JSON](https://en.wikipedia.org/wiki/JSON \"Javascript Object Notation\") to transfer data in a systematic way. This is human-readable and computer-readable.\n",
    "\n",
    "The particular format we use is a standard called [JSON:API](https://jsonapi.org). \n",
    "There is a Python package ([`jsonapi_client`](https://pypi.org/project/jsonapi-client/)) to make consuming this data easy. We're using it here.\n",
    "\n",
    "The MGnify API has a \"browsable interface\", which is a human-friendly way of exploring the API. The URLs for the browsable API are exactly the same as you'd use in a script or code; but when you open those URLs in a browser you see a nice interface. Find it here: [https://www.ebi.ac.uk/metagenomics/api/v1/](https://www.ebi.ac.uk/metagenomics/api/v1/).\n",
    "\n",
    "The MGnify API is \"paginated\", i.e. when you list some data you are given it in multiple pages. This is because there can sometimes by thousands of results. Thankfully `jsonapi_client` handles this for us.\n",
    "\n",
    "<span style=\"background-color:yellow; padding: 2px\">\n",
    "    The MGnify API has <a href=\"https://en.wikipedia.org/wiki/Rate_limiting\">rate limits</a>, to make sure the service isn't brought down by one rogue script. \n",
    "    If you list a lot of things quickly, it will slow down your responses. \n",
    "    This is especially strict when you use a Python script to call it. Consider adding pauses if you write your own scripts later!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa11c78-1ea9-4a98-ada5-dd82a5f50a08",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Example\n",
    "The MGnify website has a list of [\"Super Studies\"](https://www.ebi.ac.uk/metagenomics/browse) (collections of studies that together represent major research efforts or collaborations).\n",
    "\n",
    "What the website is actually showing, is the data from an API endpoint (i.e. specific resource within the API) that lists those. It's here: [api/v1/super-studies](https://www.ebi.ac.uk/metagenomics/api/v1/super-studies). Have a look.\n",
    "\n",
    "Here is an example of some Python code, using two popular packages that let us write a short tidy piece of code:\n",
    "\n",
    "![Do this step](assets/action.png) **Click into the next cell, and press `shift + return` (or click the ▶ icon on the menubar at the top) to run it.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b684884-80b1-4bef-a5dc-634f6e7bfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'super-studies'\n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    resources = map(lambda r: r.json, mgnify.iterate(endpoint))\n",
    "    resources = pd.json_normalize(resources)\n",
    "    resources.to_csv(f\"{endpoint}.csv\")\n",
    "resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70c5920-17f3-4a8a-a68d-9d40d7f8ef9b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Line by line explanation\n",
    "\n",
    "```python\n",
    "### The packages were already imported, but if you wanted to use this snippet on it's own as a script you would import them like this:\n",
    "from jsonapi_client import Session as APISession\n",
    "import pandas as pd\n",
    "###\n",
    "\n",
    "\n",
    "endpoint = 'super-studies'\n",
    "# An \"endpoint\" is the specific resource within the API which we want to get data from. \n",
    "# It is the a URL relative to the \"server base URL\" of the API, which for MGnify is https://www.ebi.ac.uk/metagenomics/api/v1.\n",
    "# You can find the endpoints in the API Docs https://www.ebi.ac.uk/metagenomics/api/docs/ \n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    # Calling \"APISession\" is enabling a connection to the MGnify API, that can be used multiple times. \n",
    "    # The `with...as mgnify` syntax is a Python \"context\". \n",
    "    # Everything inside the `with...` block (i.e. indented below it) can use the `APISession` which we've called `mgnify` here. \n",
    "    # When the `with` block closes (the indentation stops), the connection to the API is nicely cleaned up for us.\n",
    "    \n",
    "    resources = map(lambda r: r.json, mgnify.iterate(endpoint))\n",
    "    # `map` applies a function to every element of an iterable - so do something to each thing in a list.\n",
    "    # Remember we said the API is paginated? \n",
    "    # `mgnify.iterate(endpoint)` is a very helpful function that loops over as many pages of results as there are.\n",
    "    # `lambda r: r.json` is grabbing the JSON attribute from each Super Study returned from the API.\n",
    "    # All together, this makes `resources` be a bunch of JSON representations we could loop through, each containing the data of a Super Study.\n",
    "    \n",
    "    resources = pd.json_normalize(resources)\n",
    "    # `pd` is the de-facto shorthand for the `pandas` package - you'll see it anywhere people are using pandas.\n",
    "    # The `json_normalize` function takes \"nested\" data and does its best to turn it into a table.\n",
    "    # You can throw quite strange-looking data at it and it usually does something sensible.\n",
    "    \n",
    "    resources.to_csv(f\"{endpoint}.csv\")\n",
    "    # Pandas has a built-in way of writing CSV (or TSV, etc) files, which is helpful for getting data into other tools.\n",
    "    # This writes the table-ified Super Study list to a file called `super-studies.csv`.\n",
    "    \n",
    "resources\n",
    "# In a Jupyter notebook, you can just write a variable name in a cell (or the last line of a long cell), and it will print it.\n",
    "# Jupyter knows how to display Pandas tables (actually called \"DataFrames\", because they are More Than Just Tables ™) in a pretty way.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b2872c-9e84-4c91-8cb9-1216f2099986",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Day 3 Tasks\n",
    "## Task 1 - list Genome Catalogues\n",
    "![Do this step](assets/action.png) **In the cell below, complete the Python code to fetch the list of [Genome Catalogues from the MGnify API](https://www.ebi.ac.uk/metagenomics/api/v1/genome-catalogues), and show them in a table.**\n",
    "\n",
    "(Note that there may only be one catalogue in the list right now, that is correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048ecc9a-8bf7-4a09-8df5-f5551122b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case we skipped the API recap, make sure packages are imported\n",
    "import pandas as pd\n",
    "from jsonapi_client import Session as APISession\n",
    "import time\n",
    "import tarfile\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bece0caa-1ab1-4626-ad6f-4680d8fd334a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete this code\n",
    "\n",
    "endpoint = \n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    catalogues = \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27532217-e304-4d7f-b89b-7a0921833020",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution\n",
    "\n",
    "Unhide these cells to see a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d16709-2677-44e8-94a4-f16e0682bb8a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint = 'genome-catalogues'\n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    catalogues = map(lambda r: r.json, mgnify.iterate(endpoint))\n",
    "    catalogues = pd.json_normalize(catalogues)\n",
    "catalogues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa460ee7-f93d-4388-b044-6c20e07501e2",
   "metadata": {},
   "source": [
    "## Task 2 - list Genomes\n",
    "Each catalogue contains a much larger list of Genomes.\n",
    "\n",
    "![Do this step](assets/action.png)  **In the cell below, complete the Python code to fetch the list of [Genomes from the MGnify API](https://www.ebi.ac.uk/metagenomics/api/v1/genome-catalogues), and show them in a table.**\n",
    "\n",
    "(Note that there are quite a lot of pages of data, so this will take a minute to run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88521094-fc50-4e4f-b0fc-8651a1207c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_id = 'human-oral-v1-0\n",
    "endpoint = f'genome-catalogues/{catalogue_id}/genomes'  # a Python f-string inserts the value of a variable into the string where that variable name appears inside {..}\n",
    "\n",
    "with           as mgnify:\n",
    "    genomes = \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "genomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e15f6c-5ab8-4d0c-a921-405cc22b0a0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution\n",
    "Unhide these cells to see a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91c0456-3c0d-4e54-bddb-fac3d233e56d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "catalogue_id = 'human-oral-v1-0'\n",
    "endpoint = f'genome-catalogues/{catalogue_id}/genomes'  # a Python f-string inserts the value of a variable into the string where that variable name appears inside {..}\n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    genomes = map(lambda r: r.json, mgnify.iterate(endpoint))\n",
    "    genomes = pd.json_normalize(genomes)\n",
    "genomes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a2ab27-3a74-404e-a917-8024d7fecaae",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Task 3 - search Genome Catalogues using the website\n",
    "[MGnify Genomes](https://www.ebi.ac.uk/metagenomics/browse#genomes) offers two ways to search for Genomes in a MAG Catalogues.\n",
    "\n",
    "### Search using a gene\n",
    "1. ![Do this step](assets/action.png) Go to [the MGnify Genomes webpage](https://www.ebi.ac.uk/metagenomics/browse#genomes) and open the latest version of the Human Gut catalogue.\n",
    "2. Imagine you've got some sequence of interest and want to see whether it is in the Gut Genome Catalogue. \n",
    "    * ![Do this step](assets/action.png)  **Use the following sequence, and the \"Search by gene\" tool** to discover whether is it likely to from a species in the gut.\n",
    "\n",
    "    ```\n",
    "    GGAGTGCGGCGGAAAGTTAACCTATGCCGGACCCTGCGGGAATCCAGCTGCGTTCGAACAAGCAACCAACATATATATCTGAATTTGGATGTGGTGGGCACTTTGT\n",
    "    TGTTAGGCGCTTTGAGGTGCGAGTGACACTTTGGGGTGCGCGGAGCCCTGGGTTGGGTCGATGATTTGGGATGAGCTTCTTACTTAGGTGAAGAGGGGCTTTATGG\n",
    "    CTGAGAGGTAGTCTTTGGCTACGTCGGCTTTATCTGCTTGGAAATTGTGCCAGGCCCACCATTGGACCATTCCTACGAAGCTTGAGGCTATGTGGTGTAGTAGGAA\n",
    "    GCTTCGGTCCATGGTGGCGGCGGGGCCGTTTGGGTCGGTAGGGACGGTTTCGGCTGCTCGGGCCATGATGGTCTTGCGGAGGCTGTCGGCGAAGACGCGTGAACCG\n",
    "    GCGCCGGCTACCAGTGCCCGTACACCCTGACGGCGCTCCCAGAGGTTGTTGAGGATATGCTCGACCTGTACGAGTGGGTCATCGAGGGGCGTACCGTCATCGTCGA\n",
    "    GGGCATGGGTGCAGATATCGCGCACGAGCTCAGCGAGCAGGTCATCTTTGCTTTTGAAGAGGCCGTAGAACGTGGCCCGACCCACATGGGCGCGAGCGATGATGTC\n",
    "    GCCGACGGTGATCTTGCCGTAGTCCTCTTCGCGCAGCAGCTCGGAGAACGCCGCGACGATCGCGGCGCGGCTTTTGGCCTTGCGGGCATCCATGGCTATGCGTCCG\n",
    "    CGTCAACGAGCAGACAGCGGAGCGTCCCGGAGCAGCCCTCGTAGGGGCGCTTGCCGGCGCCGTAGCCGACGGCTTCGATGCGGTAGCGTGAGGGCAGTTGGTCGGA\n",
    "    CGTGCGCAGAGCAGTGACGATGGCGGCGCCCGTGGGCGTCACGAGCTCGCCGGCGACCGGTGCAGGCGTGAGGGCGATATTGCCCGCCTGGCACAGGTTGACGACA\n",
    "    GCGGGGACGGGAATGGGCATGAGGCCGTGGGCGCAGCGAATGGCGCCGTGGCCCTCGAAAAGCGAC\n",
    "    ```\n",
    "\n",
    "    * ![Consider this](assets/question.png) This search compares [k-mers](https://en.wikipedia.org/wiki/K-mer) of length 31, from your search sequence to every genome in the catalogue. Look at the `% Kmers found` column in the results, and the BLAST score *p* values. **Do you think the top hit is a certain match?**\n",
    "    * ![Consider this](assets/question.png) Click the Genome Accession of the top hit. Browse the available information for that Genome. What do you think the role of this species is in the human gut?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f878bc5e-2b73-438f-b905-193208935ee7",
   "metadata": {},
   "source": [
    "### Search using a genome\n",
    "For the last couple of tasks, you will need your binned MAGs generated on Day 2 of the course.\n",
    "If you didn't finish that practical, then you can copy some we made from the `penelopeprime` shared drive.\n",
    "\n",
    "1. ![Do this step](assets/action.png)  Put your binned MAG fasta files in the folder `/home/training/mags`.\n",
    "    * If you don't have them, run this command in a Terminal: `cp -r /media/penelopeprime/Metagenomics-Feb22/Day3/example-mag-bins/* ~/mags`\n",
    "2. ![Do this step](assets/action.png)  Go back to [the MGnify Genomes webpage](https://www.ebi.ac.uk/metagenomics/browse#genomes) and open the latest version of the Human Gut catalogue.\n",
    "3. ![Do this step](assets/action.png)  Pick one of your binned MAGs, and use its FASTA file with the \"Search by MAG\" tool on the website to see whether your MAG is similar to any of those in the catalogue.\n",
    "    * The query might take a couple of minutes to be queued and run.\n",
    "    * Once it finishes, look at the results table. \n",
    "        * This search uses [sourmash](https://sourmash.readthedocs.io/en/latest/) to find how much of your query metagenome is contained by target genomes in the MAG catalogue.\n",
    "        * A result where the best match shows \"60% query covered\" means 60% of the query MAG's partitions were found in the best matching catalogue MAG.\n",
    "        * Download the CSV file of all the matches (there is an icon in the results table).\n",
    "        * The [Sourmash documentation](https://sourmash.readthedocs.io/en/latest/classifying-signatures.html#appendix-a-how-sourmash-gather-works) explains the columns in this table.\n",
    "4. ![Do this step](assets/action.png)  Calculate the total ammount (i.e. the `sum`) of your query MAG that is covered by MAGs from the catalogue, by fixing the second half of code snippet (adding a calculation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3379ab-aaca-4981-862c-d63735a86110",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### FIX ME #####\n",
    "downloaded_csv_file = '/home/training/downloads/                                           .csv'\n",
    "##################\n",
    "\n",
    "sourmash_results = pd.read_csv(downloaded_csv_file)\n",
    "display(sourmash_results)  # this shows the CSV table, loaded into a Pandas dataframe, in a pretty format. `display` is a special Jupyter function, that won't work in a regular python script.\n",
    "\n",
    "query_contained_by_best_match = sourmash_results.f_unique_to_query.max()\n",
    "print(f'The best matching MAG contained {query_contained_by_best_match * 100}% of the query’s k-mers.')\n",
    "\n",
    "\n",
    "##### FIX ME #####\n",
    "query_contained_by_all_matches = \n",
    "print(f'All matching MAGs together contained {query_contained_by_all_matches * 100}% of the query’s k-mers.')\n",
    "##################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59870df2-dae7-4b4a-90cb-fb9e244eebd4",
   "metadata": {},
   "source": [
    "![Consider this](assets/question.png) Do you think this containment fraction means your MAG is novel, or is it well-represented by genomes in the MGnify catalogue? How complete and contaminated do you think your MAGs are? How would a low completeness (say 50%) affect the threshold you’d be looking for to consider your MAG represented by the catalogue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cfbe74-fe92-49ed-a4b2-6e9883aea1a7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Solution\n",
    "Unhide these cells to see a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75277d06-baf3-40a6-884f-453dcb3cce9e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "downloaded_csv_file = max(Path('/home/training/Downloads').glob('*.csv'), key=os.path.getctime)\n",
    "\n",
    "sourmash_results = pd.read_csv(downloaded_csv_file)\n",
    "display(sourmash_results)  # this shows the CSV table, loaded into a Pandas dataframe, in a pretty format. `display` is a special Jupyter function, that won't work in a regular python script.\n",
    "\n",
    "query_contained_by_best_match = sourmash_results.f_unique_to_query.max()\n",
    "print(f'The best matching MAG contained {query_contained_by_best_match * 100}% of the query’s k-mers.')\n",
    "\n",
    "query_contained_by_all_matches = sourmash_results.f_unique_to_query.sum()\n",
    "print(f'All matching MAGs together contained {query_contained_by_all_matches * 100}% of the query’s k-mers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7786db87-9159-4271-a989-9a172918d95d",
   "metadata": {},
   "source": [
    "## Task 4 - Find out whether your MAGs are novel, using the API\n",
    "In this final task, we will combine the API skills you’ve gained with your knowledge of the MAG search mechanism.\n",
    "\n",
    "Imagine you created more than a couple of MAGs, following the process of Day 2 but using a big dataset and a high performance computing cluster. Now, you want to see if any of them are novel or are they well covered by a catalogue on the MGnify resource. It will be a pain to do all that by hand! You can upload a directory of a few MAGs on the website, but for 100s or 1000s, you need to use the API.\n",
    "\n",
    "Follow along and fill in the missing pieces of code to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bcbc20-2ae2-4bc3-8e8d-5c915b7eed0c",
   "metadata": {},
   "source": [
    "We need to compute a \"sketch\" for each Genome, using Sourmash. On the website this happens in your browser. To use the API, we do it using the [sourmash](https://sourmash.readthedocs.io/en/latest/) package. We will also use [Biopython’s SeqIO](https://biopython.org/wiki/SeqRecord) package to read the FASTA files in Python code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e6803e-a10c-4616-8411-2fe0c512ad1f",
   "metadata": {},
   "source": [
    "**Most of the code here is completed for you, because it would take some time to learn how to put it all together. You can follow the code comments, or just press `shift + enter` in each cell to run it and come back to study it later.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f0f43-63cf-4146-8494-3dc872b1a798",
   "metadata": {},
   "source": [
    "### Load up our MAGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed2fe7e-1160-486b-8b15-6fdaaaf07fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you didn't have these packages installed, you'd need to run \"pip install pandas biopython sourmash\"\n",
    "\n",
    "import sourmash\n",
    "from Bio import SeqIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34987b2-efc5-4a71-ab6e-6a82e3d0bf34",
   "metadata": {},
   "source": [
    "We’ll find the filepath for all of our \"new\" MAGs.\n",
    "\n",
    "![Do this step](assets/action.png) Edit the value of `my_mags_folder` if you put your MAGs somewhere different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6267f2-212a-4def-a111-9e150e7c4566",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_mags_folder = Path('/home/training/mags')\n",
    "\n",
    "# pathlib is a handy standard Python library for finding files and directories\n",
    "my_mags_files = list(my_mags_folder.glob('*.fa*'))\n",
    "my_mags_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff04a3f-7da9-46f0-80f7-a46955863b86",
   "metadata": {},
   "source": [
    "### Calculate sourmash \"sketches\" to search against the MGnify catalogue\n",
    "We’ll compute a sourmash sketch for each MAG. A sketch goes into a signature, that we will use for searching. The signature is a sort of collection of hashes that are well suited for calculating the *containment* of your MAGs within the catalogue's MAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b20f63-4286-4b06-94a2-decf47ab0e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for mag_file in my_mags_files:\n",
    "    # the sourmash parameters are chosen to match those used within MGnify\n",
    "    sketch = sourmash.MinHash(n=0, ksize=31, scaled=1000)\n",
    "    \n",
    "    # a fasta file may have multiple records in it. add them all to the sourmash signature.\n",
    "    for index, record in enumerate(SeqIO.parse(mag_file, 'fasta')):\n",
    "        sketch.add_sequence(str(record.seq))\n",
    "\n",
    "    # save the sourmash sketch as a \"signature\" file\n",
    "    sig = sourmash.SourmashSignature(sketch, name=record.name or mag_file.stem)\n",
    "    with open(mag_file.stem + '.sig', 'wt') as fp:\n",
    "        sourmash.save_signatures([sig], fp)\n",
    "\n",
    "# check what signature files we've created.\n",
    "# using ! in Jupyter lets you run a shell command. It is handy for quick things like pwd and ls.\n",
    "!ls *.sig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0949d10-5e0d-4171-9b24-8b679bee1795",
   "metadata": {},
   "source": [
    "### Submit a search job to the MGnify API\n",
    "We’ll call the MGnify API with all of our sketches.\n",
    "There is an endpoint for this (the same one used by the website).\n",
    "\n",
    "In this case, we need to **send** data to the API (not just fetch it). This is called \"POST\"ing data in the API world. \n",
    "\n",
    "This part of the API is quite specialized and so is not a formal JSON:API, so we use the more flexible [requests](https://docs.python-requests.org/en/master/) Python package to communicate with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa57664-0850-4d5c-8d70-34943187402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8e2f87-8f5f-4d79-a768-b4741073c21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://www.ebi.ac.uk/metagenomics/api/v1/genomes-search/gather'\n",
    "catalogue_id = 'human-gut-v2-0'  # You could change this to any other catalogue ID from the MGnify website, if you use this in the future.\n",
    "\n",
    "# Create a list of file uploads, and attach them to the API request\n",
    "signatures = [open(mag.stem + '.sig', 'rb') for mag in my_mags_files]\n",
    "sketch_uploads = [('file_uploaded', signature) for signature in signatures]\n",
    "\n",
    "# Send the API request - it specifies which catalogue to search against and attaches all of the signature files.\n",
    "submitted_job = requests.post(endpoint, data={'mag_catalog': catalogue_id}, files=sketch_uploads).json()\n",
    "\n",
    "map(lambda fp: fp.close(), signatures)  # tidy up open file pointers\n",
    "\n",
    "print(submitted_job)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423c785c-100e-45ed-8a19-0a4289327c88",
   "metadata": {},
   "source": [
    "### Wait for our results to be ready\n",
    "As you can see in the printed `submitted_job` above, a `status_URL` was returned in the response from submitting the job via the API.\n",
    "Since the job will be in a queue, we must poll this `status_URL` to wait for our job to be completed.\n",
    "We’ll check every 2 seconds until ALL of the jobs are finished."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3578bd-4758-4bee-82da-cc839c01b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_done = False\n",
    "while not job_done:\n",
    "    print('Checking status...')\n",
    "    status = requests.get(submitted_job['data']['status_URL'])\n",
    "    # the status_URL is just another API endpoint that's unique for our search job\n",
    "    \n",
    "    queries_done = {sig['job_id']: sig['status'] for sig in status.json()['data']['signatures']}\n",
    "    job_done = all(map(lambda q: q == 'SUCCESS', queries_done.values()))\n",
    "    if not job_done:\n",
    "        print('Still waiting for jobs to complete. Current status of jobs')\n",
    "        print(queries_done)\n",
    "        print('Will check again in 2 seconds')\n",
    "        time.sleep(2)\n",
    "\n",
    "print('All finished!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92c3f6-d731-4333-ab92-b0ee6641a9c2",
   "metadata": {},
   "source": [
    "### Download all of the search results\n",
    "Now, we need to fetch the results. We can grab these all at once, as a compressed archive (a `.tgz` file), via the top level `results_url` from the `status` endpoint's response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb730dc-7240-4741-aef1-ba976a4a6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_endpoint = status.json()['data']['results_url']\n",
    "print(f'Will fetch results from {results_endpoint}')\n",
    "\n",
    "results_response = requests.get(results_endpoint, stream=True)\n",
    "assert results_response.status_code == 200\n",
    "\n",
    "with open('mag_novelty_results.tgz', 'wb') as tarball:\n",
    "    tarball.write(results_response.raw.read())\n",
    "\n",
    "!ls *.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bdaa00-61af-4fec-b23a-33ce3b19feab",
   "metadata": {},
   "source": [
    "### Make a table with our search results\n",
    "The tarball we just downloaded contains `.csv` files – one for each query, so one for each of your MAGs.\n",
    "We can load them all up and put them into a single Pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ca56cc-88f8-4d80-bd69-464cc569e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll make an array of all the tables and concatenate them using pandas\n",
    "results_tables = []\n",
    "\n",
    "# We need to translate the Job IDs (assigned by the MGnify API) back to the name of the each MAG.\n",
    "# This just creates that map, so in the combined table we know what result applies to what MAG.\n",
    "job_to_mag_filename = {sig['job_id']: sig['filename'].rstrip('.sig') for sig in status.json()['data']['signatures']}\n",
    "\n",
    "# Python has built-in support for tarfiles, so we can pull the CSVs from it straight into Pandas.\n",
    "with tarfile.open('mag_novelty_results.tgz', 'r:gz') as tarball:\n",
    "    for results_csv in tarball.getmembers():\n",
    "        results_table = pd.read_csv(tarball.extractfile(results_csv))\n",
    "        \n",
    "        # add a column to the table with the MAG filename on every row\n",
    "        job_id = results_csv.name.rstrip('.csv')\n",
    "        results_table['query_mag'] = job_to_mag_filename[job_id]\n",
    "        results_tables.append(results_table)\n",
    "\n",
    "# Stick all the tables together (same columns, so we're stacking the rows here)\n",
    "mag_novelty_results = pd.concat(results_tables, ignore_index=True)\n",
    "\n",
    "mag_novelty_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bd234-dc2d-4ff1-afef-1a9b4ee8c2d1",
   "metadata": {},
   "source": [
    "### Compute statistics on the search results\n",
    "We can find the number of matches for each MAG by grouping and counting the table rows.\n",
    "\n",
    "We will use Pandas [groupby](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) for this. GroupBy lets you calculate an \"aggregate\" statistic on each group of rows, where the groups are usually defined by having the same value of some column. In our case, we're grouping by the `query_mag` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feeecb03-67b0-4d22-b180-19528330dc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_novelty_results.groupby('query_mag').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855a7eab-0a74-4ae0-bc5a-c0b18ba7af2b",
   "metadata": {},
   "source": [
    "![Do this step](assets/action.png) **Calculate the apparent novelty of each of your MAGs**. \n",
    "Use a `groupby`, and remember you’re trying to find out how much of each MAG is contained by ALL of the matches from the catalogue.\n",
    "That is the `f_unique_to_query` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9796eb67-2a7c-4652-bf41-48fcab42dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mag_novelty_results.groupby("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa10684-e959-4e5e-a654-41dff92e5553",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution\n",
    "Unhide these cells to see a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2679dc43-ae9a-481a-a75b-770fb490b762",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mag_novelty_results.groupby('query_mag').sum().sort_values('f_unique_to_query')\n",
    "# We've added the .sort_values() for bonus points! This just orders the table by that column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c914c70e-88a9-478e-81b5-bb4f657a8f8e",
   "metadata": {},
   "source": [
    "### Examine properties of matched MAGs, using data from the MGnify FTP\n",
    "Finally, let’s look at the matched catalogue genomes for one of our MAGs.\n",
    "\n",
    "Since we're looking at *containment* of your MAGs within a catalogue, the sourmash search looks at **all** genomes in the MGnify catalogue, not just the cluster/species representatives that are browsable on the website and accessible by the API.\n",
    "\n",
    "The [EBI FTP site for MGnify](http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_genomes/) lets you access the full catalogue dataset.\n",
    "There is a file for each catalogue (`genomes-all_metadata.tsv`) with some basic information about all of the genomes. This file also contains the mapping of each genome to its Species Representative (as shown on the website and API).\n",
    "\n",
    "We’ve downloaded it to the shared drive (`penelopeprime`) to save you a few minutes waiting for it.\n",
    "\n",
    "For this task we will:\n",
    "- find which of your MAGs has the most matches from the sourmash search\n",
    "- extract the ID of those matches, from the search results\n",
    "- find the corresponding rows in the big metadata table we fetched from the FTP\n",
    "- plot statistics about the matched MGnify genomes\n",
    "\n",
    "![Do this step](assets/action.png) **Explore, and complete, the code in the following cells to do this**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021da96c-9d79-46f4-a57f-1ded9c554a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_metadata = pd.read_csv('/media/penelopeprime/Metagenomics-Feb22/Day3/genomes-all_metadata.tsv', sep='\\t', index_col='Genome')\n",
    "# the (big) file was downloaded from http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_genomes/human-gut/v2.0/genomes-all_metadata.tsv\n",
    "all_genomes_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5e7375-c8f6-40aa-970e-617070c8b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the mag with most matches (use a .groupby() and an aggregate statistic that counts the rows of each group)\n",
    "#   .f_unique_to_query.idxmax() finds the INDEX (label) corresponding to the maximum vaule of the f_unique_to_query column. \n",
    "#   Note that any column would do since we're just counting!\n",
    "#   We want mag_with_most_matches to be something like \"bin.2\".\n",
    "\n",
    "########################################### COMPLETE ME ###########################################\n",
    "mag_with_most_matches = mag_novelty_results.                            .f_unique_to_query.idxmax()\n",
    "###################################################################################################\n",
    "\n",
    "print(f'{mag_with_most_matches} has the most matches')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38117927-14b7-439c-bc15-31c0d82d60c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the matched genome name from the filepath that's returned in the results\n",
    "mag_novelty_results['match_genome_name'] = mag_novelty_results.apply(lambda result: result['name'].split('/')[-1].rstrip('.fa'), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c78b23-fe53-4517-bfb2-6c0be711dcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the search results for the MAG we're interested in – the one with the most matches\n",
    "matches_of_interest = mag_novelty_results[mag_novelty_results.query_mag==mag_with_most_matches]\n",
    "\n",
    "# Use the newly created \"match_genome_name\" as the table index\n",
    "# we can do this because it is now unique (the same MGnify genome can't match twice for the same query)\n",
    "matches_of_interest.set_index('match_genome_name', inplace=True)\n",
    "matches_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3a6cda-331b-45cd-a5b2-9aa511f42eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas has a powerful \"join\" feature. Since we now have two tables of data indexed by the MGnify genome IDs, we can join their columns together\n",
    "matches_of_interest = matches_of_interest.join(all_genomes_metadata)\n",
    "\n",
    "matches_of_interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85d3a7-d4e0-47da-9767-f7470ad0cf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s see the completeness of the MGnify genomes that seem to contain our most-matched MAG\n",
    "plt.hist(matches_of_interest.Completeness)\n",
    "plt.xlabel('Completeness \\ %')\n",
    "plt.ylabel('Number of genomes')\n",
    "plt.title(f'Completeness histogram for MGnify Genomes matching {mag_with_most_matches}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede3c531-fedc-4583-9b6f-0a43d12248c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a plot of the contamination fraction of each of the genomes that match our MAG.\n",
    "# Note that you can call `matches_of_interest.columns` to see a list of all the columns in the table.\n",
    "\n",
    "########################################### COMPLETE ME ###########################################\n",
    "plt.\n",
    "plt.xlabel('Contamination \\ %')\n",
    "plt.ylabel\n",
    "plt.title\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e3277d-92de-47cd-b966-c7b73723379d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a chart showing the which continents had how many samples matching your MAG\n",
    "########################################### COMPLETE ME ###########################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2b4587-d88d-4130-a222-67f879a2bc06",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Solution\n",
    "Unhide these cells to see a solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3453698d-da86-4ef1-9766-29c842810504",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "all_genomes_metadata = pd.read_csv('/media/penelopeprime/Metagenomics-Nov21/Day4/genomes-all_metadata.tsv', sep='\\t', index_col='Genome')\n",
    "# the (big) file was downloaded from http://ftp.ebi.ac.uk/pub/databases/metagenomics/mgnify_genomes/human-gut/v1.0/genomes-all_metadata.tsv\n",
    "\n",
    "mag_with_most_matches = mag_novelty_results.groupby('query_mag').count().f_unique_to_query.idxmax()\n",
    "print(f'{mag_with_most_matches} has the most matches')\n",
    "\n",
    "# Pull out the matched genome name from the filepath that's returned in the results\n",
    "mag_novelty_results['match_genome_name'] = mag_novelty_results.apply(lambda result: result['name'].split('/')[-1].rstrip('.fa'), axis=1)\n",
    "\n",
    "# Select the search results for the MAG we're interested in – the one with the most matches\n",
    "matches_of_interest = mag_novelty_results[mag_novelty_results.query_mag==mag_with_most_matches]\n",
    "\n",
    "# Use the newly created \"match_genome_name\" as the table index\n",
    "# we can do this because it is now unique (the same MGnify genome can't match twice for the same query)\n",
    "matches_of_interest.set_index('match_genome_name', inplace=True)\n",
    "\n",
    "# Pandas has a powerful \"join\" feature. Since we now have two tables of data indexed by the MGnify genome IDs, we can join their columns together\n",
    "matches_of_interest = matches_of_interest.join(all_genomes_metadata)\n",
    "\n",
    "# Let’s see the completeness of the MGnify genomes that seem to contain our most-matched MAG\n",
    "plt.figure(0)\n",
    "plt.hist(matches_of_interest.Completeness)\n",
    "plt.xlabel('Completeness \\ %')\n",
    "plt.ylabel('Number of genomes')\n",
    "plt.title(f'Completeness histogram for MGnify Genomes matching {mag_with_most_matches}');\n",
    "\n",
    "plt.figure(1)\n",
    "plt.hist(matches_of_interest.Contamination)\n",
    "plt.xlabel('Contamination \\ %')\n",
    "plt.ylabel('Number of genomes')\n",
    "plt.title(f'Contamination histogram for MGnify Genomes matching {mag_with_most_matches}');\n",
    "\n",
    "plt.figure(2)\n",
    "matches_of_interest.Continent.hist()\n",
    "plt.xlabel('Continent where sample was collected')\n",
    "plt.ylabel('Number of samples')\n",
    "plt.title(f'Geographical spread of samples for MGnify Genomes matching {mag_with_most_matches}');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mgnify-ebi-2021",
   "language": "python",
   "name": "mgnify-ebi-2021"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
